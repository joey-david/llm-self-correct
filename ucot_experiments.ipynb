{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAUQ-CoT Experiments & Ablations\n",
    "\n",
    "This notebook prepares calibration artifacts, runs the baseline RAUQ-triggered controller, and executes targeted ablations. Execute the cells sequentially on a GPU-enabled runtime. Update the configuration cell before running if you need to change models, datasets, or output locations. Ensure you have access to the chosen Hugging Face model (login via `huggingface-cli login` if required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install runtime dependencies (run once per environment)\n",
    "!pip install -q transformers accelerate bitsandbytes datasets evaluate scikit-learn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "REPO_ROOT = Path.cwd()\n",
    "ARTIFACTS_DIR = REPO_ROOT / \"artifacts\"\n",
    "CALIB_DIR = REPO_ROOT / \"data\" / \"calibration\"\n",
    "EVAL_DIR = ARTIFACTS_DIR / \"evals\"\n",
    "ABLATION_DIR = ARTIFACTS_DIR / \"ablations\"\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "TOKENIZER_NAME = None  # Set to override tokenizer (defaults to MODEL_NAME)\n",
    "BENCHMARK_NAME = \"gsm8k\"\n",
    "\n",
    "CALIBRATION_SIZE = None  # Number of calibration prompt/completion pairs; set to None to use all available samples\n",
    "CALIBRATION_FILE = CALIB_DIR / \"gsm8k_calibration.jsonl\"\n",
    "\n",
    "HEADS_PATH = ARTIFACTS_DIR / \"qwen25_heads.json\"\n",
    "THRESHOLD_PATH = ARTIFACTS_DIR / \"qwen25_theta.json\"\n",
    "\n",
    "MAX_NEW_TOKENS = None\n",
    "ALPHA = 0.3\n",
    "DEVICE = \"cuda\"\n",
    "EVAL_LIMIT = None  # Set to an int for smoke tests (e.g., 32)\n",
    "SEED = 0\n",
    "\n",
    "for path in (ARTIFACTS_DIR, CALIB_DIR, EVAL_DIR, ABLATION_DIR):\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "os.environ[\"PYTHONPATH\"] = str(REPO_ROOT)\n",
    "os.environ.setdefault(\"HF_HOME\", str(REPO_ROOT / \".hf_cache\"))\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"Repo root: {REPO_ROOT}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Calibration file: {CALIBRATION_FILE}\")\n",
    "print(f\"Artifacts dir: {ARTIFACTS_DIR}\")\n",
    "print(f\"Evaluation limit: {EVAL_LIMIT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from ucot.data.benchmarks import load_benchmark\n",
    "\n",
    "def prepare_calibration_file() -> None:\n",
    "    if CALIBRATION_FILE.exists():\n",
    "        print(f\"Calibration file already present: {CALIBRATION_FILE}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        calibration_samples = load_benchmark(BENCHMARK_NAME, split=\"train\", limit=CALIBRATION_SIZE)\n",
    "    except Exception as exc:\n",
    "        print(f\"Train split unavailable ({exc}); falling back to default evaluation split.\")\n",
    "        calibration_samples = load_benchmark(BENCHMARK_NAME, limit=CALIBRATION_SIZE)\n",
    "\n",
    "    if not calibration_samples:\n",
    "        raise ValueError(\"No calibration samples fetched; update BENCHMARK_NAME or CALIBRATION_SIZE.\")\n",
    "\n",
    "    with CALIBRATION_FILE.open(\"w\") as fp:\n",
    "        for sample in calibration_samples:\n",
    "            payload = {\"prompt\": sample.prompt, \"completion\": sample.reference}\n",
    "            fp.write(json.dumps(payload) + \"\\n\")\n",
    "\n",
    "    print(f\"Wrote {len(calibration_samples)} calibration examples to {CALIBRATION_FILE}\")\n",
    "\n",
    "prepare_calibration_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucot.config import HeadSelectionConfig\n",
    "from ucot.head_selection import select_uncertainty_heads\n",
    "\n",
    "head_config = HeadSelectionConfig(\n",
    "    calibration_paths=[CALIBRATION_FILE],\n",
    "    model_name=MODEL_NAME,\n",
    "    tokenizer_name=TOKENIZER_NAME,\n",
    "    output_path=HEADS_PATH,\n",
    "    num_examples=CALIBRATION_SIZE,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "head_result = select_uncertainty_heads(head_config)\n",
    "print(f\"Saved head selection to {HEADS_PATH}\")\n",
    "print(f\"Layers used ({len(head_result.layers_used)}): {head_result.layers_used}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucot.config import ThresholdTrainingConfig\n",
    "from ucot.threshold import train_threshold\n",
    "\n",
    "max_pairs = None if CALIBRATION_SIZE is None else min(2048, CALIBRATION_SIZE)\n",
    "threshold_config = ThresholdTrainingConfig(\n",
    "    calibration_paths=[CALIBRATION_FILE],\n",
    "    model_name=MODEL_NAME,\n",
    "    tokenizer_name=TOKENIZER_NAME,\n",
    "    head_indices_path=HEADS_PATH,\n",
    "    output_path=THRESHOLD_PATH,\n",
    "    alpha=ALPHA,\n",
    "    max_samples=max_pairs,\n",
    "    device=DEVICE,\n",
    ")\n",
    "threshold_result = train_threshold(threshold_config)\n",
    "print(f\"Learned theta: {threshold_result.theta:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d98e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from statistics import mean\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "\n",
    "from ucot.config import ControllerConfig, RAUQConfig\n",
    "from ucot.controller import RAUQController\n",
    "from ucot.data.benchmarks import load_benchmark\n",
    "from ucot.experiments.metrics import METRICS, exact_match\n",
    "from ucot.rauq import RAUQScorer\n",
    "from ucot.threshold import ThresholdResult\n",
    "from ucot.uncertainty import EntropyScorer, LogitMarginScorer, RAUQScorerWrapper\n",
    "from ucot.utils.model import load_model\n",
    "\n",
    "_LOADED_MODEL = None\n",
    "\n",
    "def get_loaded_model():\n",
    "    global _LOADED_MODEL\n",
    "    if _LOADED_MODEL is None:\n",
    "        _LOADED_MODEL = load_model(\n",
    "            model_name=MODEL_NAME,\n",
    "            tokenizer_name=TOKENIZER_NAME,\n",
    "            device=DEVICE,\n",
    "        )\n",
    "    return _LOADED_MODEL\n",
    "\n",
    "def ensure_cuda_sync():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def compute_summary(records, latencies):\n",
    "    accuracy = mean(r[\"correct\"] for r in records) if records else 0.0\n",
    "    avg_generated = mean(r[\"generated_tokens\"] for r in records) if records else 0.0\n",
    "    avg_total = mean(r[\"total_tokens\"] for r in records) if records else 0.0\n",
    "    avg_triggers = mean(r[\"triggers\"] for r in records) if records else 0.0\n",
    "    tokens_total = sum(r[\"generated_tokens\"] for r in records)\n",
    "    correct_total = sum(r[\"correct\"] for r in records)\n",
    "    tokens_per_correct = tokens_total / max(correct_total, 1)\n",
    "    avg_latency = mean(latencies) if latencies else 0.0\n",
    "    return {\n",
    "        \"samples\": len(records),\n",
    "        \"accuracy\": accuracy,\n",
    "        \"avg_generated_tokens\": avg_generated,\n",
    "        \"avg_total_tokens\": avg_total,\n",
    "        \"avg_triggers\": avg_triggers,\n",
    "        \"tokens_per_correct\": tokens_per_correct,\n",
    "        \"total_generated_tokens\": tokens_total,\n",
    "        \"avg_latency_sec\": avg_latency,\n",
    "    }\n",
    "\n",
    "def print_summary(name, summary, out_path):\n",
    "    print(f\"=== {name} ===\")\n",
    "    for key, value in summary.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"{key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "    print(f\"Artifacts: {out_path}\")\n",
    "\n",
    "def run_vanilla_greedy(experiment_name: str, temperature: float = 0.0, top_p: float = 1.0, show_progress: bool = True):\n",
    "    loaded = get_loaded_model()\n",
    "    model = loaded.model\n",
    "    tokenizer = loaded.tokenizer\n",
    "\n",
    "    dataset = load_benchmark(BENCHMARK_NAME, limit=EVAL_LIMIT)\n",
    "    metric_fn = METRICS.get(BENCHMARK_NAME, exact_match)\n",
    "    results = []\n",
    "    latencies = []\n",
    "\n",
    "    progress_bar = None\n",
    "    if show_progress:\n",
    "        try:\n",
    "            from tqdm.auto import tqdm\n",
    "            progress_bar = tqdm(total=len(dataset), desc=f\"{experiment_name}\", unit=\"sample\")\n",
    "        except ImportError:\n",
    "            progress_bar = None\n",
    "\n",
    "    model.eval()\n",
    "    for sample in dataset:\n",
    "        inputs = tokenizer(sample.prompt, return_tensors=\"pt\").to(model.device)\n",
    "        ensure_cuda_sync()\n",
    "        start = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                do_sample=temperature > 0.0,\n",
    "                temperature=temperature if temperature > 0.0 else 1.0,\n",
    "                top_p=top_p,\n",
    "            )\n",
    "        ensure_cuda_sync()\n",
    "        latency = time.perf_counter() - start\n",
    "        completion_ids = outputs[:, inputs[\"input_ids\"].shape[1]:]\n",
    "        completion = tokenizer.decode(completion_ids[0], skip_special_tokens=True)\n",
    "        is_correct = bool(metric_fn({\"reference\": sample.reference}, completion))\n",
    "        results.append({\n",
    "            \"id\": sample.metadata.get(\"id\") or sample.metadata.get(\"task_id\") or sample.metadata.get(\"level\"),\n",
    "            \"correct\": int(is_correct),\n",
    "            \"triggers\": 0,\n",
    "            \"generated_tokens\": int(completion_ids.shape[1]),\n",
    "            \"total_tokens\": int(outputs.shape[1]),\n",
    "            \"extra_tokens\": int(completion_ids.shape[1]),\n",
    "            \"latency_sec\": latency,\n",
    "        })\n",
    "        latencies.append(latency)\n",
    "        if progress_bar is not None:\n",
    "            progress_bar.update(1)\n",
    "\n",
    "    if progress_bar is not None:\n",
    "        progress_bar.close()\n",
    "\n",
    "    summary = compute_summary(results, latencies)\n",
    "    payload = {\"name\": experiment_name, \"summary\": summary, \"records\": results}\n",
    "    out_path = EVAL_DIR / f\"{experiment_name}.json\"\n",
    "    out_path.write_text(json.dumps(payload, indent=2))\n",
    "    print_summary(experiment_name, summary, out_path)\n",
    "    return summary\n",
    "\n",
    "def build_scorer(trigger: str, alpha: float, num_layers: int):\n",
    "    if trigger == \"rauq\":\n",
    "        rauq_config = RAUQConfig(alpha=alpha, head_indices_path=HEADS_PATH, device=DEVICE)\n",
    "        base = RAUQScorer.from_config(rauq_config, num_layers=num_layers)\n",
    "        return RAUQScorerWrapper(\n",
    "            alpha=base.alpha,\n",
    "            head_indices=base.head_indices,\n",
    "            layers=base.layers,\n",
    "            eps=base.eps,\n",
    "            device=base.device,\n",
    "        )\n",
    "    if trigger == \"entropy\":\n",
    "        return EntropyScorer()\n",
    "    if trigger == \"margin\":\n",
    "        return LogitMarginScorer()\n",
    "    raise ValueError(f\"Unsupported trigger: {trigger}\")\n",
    "\n",
    "def configure_cot(controller_config, policy: str, override_length: Optional[int]):\n",
    "    if policy == \"rauq\":\n",
    "        controller_config.cot.stop_mode = \"rauq\"\n",
    "        if override_length is not None:\n",
    "            controller_config.cot.max_cot_tokens = override_length\n",
    "    elif policy == \"max20\":\n",
    "        controller_config.cot.stop_mode = \"fixed\"\n",
    "        controller_config.cot.max_cot_tokens = override_length or 20\n",
    "    elif policy == \"unlimited\":\n",
    "        controller_config.cot.stop_mode = \"none\"\n",
    "        controller_config.cot.max_cot_tokens = override_length or 200\n",
    "    elif policy == \"none\":\n",
    "        controller_config.cot.stop_mode = \"none\"\n",
    "        controller_config.cot.max_cot_tokens = override_length or 0\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown CoT policy: {policy}\")\n",
    "    controller_config.cot.cot_prefix = (\n",
    "        f\"Wait, let's quickly think step by step about this (<{controller_config.cot.max_cot_tokens} tokens).\"\n",
    "    )\n",
    "\n",
    "def run_controller_experiment(\n",
    "    experiment_name: str,\n",
    "    trigger: str = \"rauq\",\n",
    "    repair: str = \"cot\",\n",
    "    cot_policy: str = \"rauq\",\n",
    "    cot_length: Optional[int] = None,\n",
    "    rollback_depth: int = 2,\n",
    "    rollback_mode: str = \"fixed\",\n",
    "    cooldown: int = 5,\n",
    "    stability_window: int = 2,\n",
    "    max_triggers: Optional[int] = 5,\n",
    "    alpha: Optional[float] = None,\n",
    "    temperature: float = 0.0,\n",
    "    top_p: float = 1.0,\n",
    "    theta: Optional[float] = None,\n",
    "    use_learned_threshold: bool = True,\n",
    "    show_progress: bool = True,\n",
    "):\n",
    "    alpha = alpha if alpha is not None else ALPHA\n",
    "    loaded = get_loaded_model()\n",
    "    num_layers = loaded.model.config.num_hidden_layers\n",
    "    scorer = build_scorer(trigger, alpha, num_layers)\n",
    "    threshold = None\n",
    "    theta_value = theta\n",
    "\n",
    "    if use_learned_threshold:\n",
    "        threshold = ThresholdResult.load(THRESHOLD_PATH)\n",
    "        theta_value = threshold.theta if theta_value is None else theta_value\n",
    "    else:\n",
    "        if theta_value is None:\n",
    "            raise ValueError(\"Provide theta when use_learned_threshold=False\")\n",
    "\n",
    "    controller_config = ControllerConfig(\n",
    "        model_name=MODEL_NAME,\n",
    "        tokenizer_name=TOKENIZER_NAME,\n",
    "        theta=theta_value,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        alpha=alpha,\n",
    "    )\n",
    "    configure_cot(controller_config, cot_policy, cot_length)\n",
    "    controller_config.repair_strategy = repair\n",
    "    controller_config.rollback.rollback_depth = rollback_depth\n",
    "    controller_config.rollback.mode = rollback_mode\n",
    "    controller_config.rollback.cooldown = cooldown\n",
    "    controller_config.rollback.stability_window = stability_window\n",
    "    controller_config.rollback.max_triggers = max_triggers\n",
    "\n",
    "    controller = RAUQController(\n",
    "        loaded=loaded,\n",
    "        config=controller_config,\n",
    "        scorer=scorer,\n",
    "        threshold=threshold if use_learned_threshold else None,\n",
    "    )\n",
    "\n",
    "    dataset = load_benchmark(BENCHMARK_NAME, limit=EVAL_LIMIT)\n",
    "    metric_fn = METRICS.get(BENCHMARK_NAME, exact_match)\n",
    "    results = []\n",
    "    latencies = []\n",
    "\n",
    "    progress_bar = None\n",
    "    if show_progress:\n",
    "        try:\n",
    "            from tqdm.auto import tqdm\n",
    "            progress_bar = tqdm(total=len(dataset), desc=f\"{experiment_name}\", unit=\"sample\")\n",
    "        except ImportError:\n",
    "            progress_bar = None\n",
    "\n",
    "    for sample in dataset:\n",
    "        ensure_cuda_sync()\n",
    "        start = time.perf_counter()\n",
    "        output = controller.generate(sample.prompt)\n",
    "        ensure_cuda_sync()\n",
    "        latency = time.perf_counter() - start\n",
    "        is_correct = bool(metric_fn({\"reference\": sample.reference}, output.completion))\n",
    "        results.append({\n",
    "            \"id\": sample.metadata.get(\"id\") or sample.metadata.get(\"task_id\") or sample.metadata.get(\"level\"),\n",
    "            \"correct\": int(is_correct),\n",
    "            \"triggers\": len(output.trigger_events),\n",
    "            \"generated_tokens\": len(output.completion_tokens),\n",
    "            \"total_tokens\": output.total_tokens,\n",
    "            \"extra_tokens\": output.extra_tokens,\n",
    "            \"latency_sec\": latency,\n",
    "        })\n",
    "        latencies.append(latency)\n",
    "        if progress_bar is not None:\n",
    "            progress_bar.update(1)\n",
    "\n",
    "    if progress_bar is not None:\n",
    "        progress_bar.close()\n",
    "\n",
    "    summary = compute_summary(results, latencies)\n",
    "    payload = {\n",
    "        \"name\": experiment_name,\n",
    "        \"summary\": summary,\n",
    "        \"records\": results,\n",
    "        \"config\": {\n",
    "            \"trigger\": trigger,\n",
    "            \"repair\": repair,\n",
    "            \"cot_policy\": cot_policy,\n",
    "            \"cot_length\": cot_length,\n",
    "            \"rollback_depth\": rollback_depth,\n",
    "            \"rollback_mode\": rollback_mode,\n",
    "            \"cooldown\": cooldown,\n",
    "            \"stability_window\": stability_window,\n",
    "            \"max_triggers\": max_triggers,\n",
    "            \"alpha\": alpha,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p,\n",
    "            \"theta\": theta_value,\n",
    "            \"use_learned_threshold\": use_learned_threshold,\n",
    "        },\n",
    "    }\n",
    "    out_path = ABLATION_DIR / f\"{experiment_name}.json\"\n",
    "    out_path.write_text(json.dumps(payload, indent=2))\n",
    "    print_summary(experiment_name, summary, out_path)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 0 \u2014 Plain greedy decoding (no RAUQ controller)\n",
    "greedy_summary = run_vanilla_greedy(\"baseline_greedy\")\n",
    "greedy_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1 \u2014 RAUQ-triggered controller with CoT repair (baseline)\n",
    "baseline_summary = run_controller_experiment(\n",
    "    experiment_name=\"rauq_cot_baseline\",\n",
    "    trigger=\"rauq\",\n",
    "    repair=\"cot\",\n",
    "    cot_policy=\"rauq\",\n",
    "    rollback_depth=2,\n",
    "    rollback_mode=\"fixed\",\n",
    "    max_triggers=5,\n",
    ")\n",
    "baseline_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2 \u2014 RAUQ controller without micro-CoT repair\n",
    "no_cot_summary = run_controller_experiment(\n",
    "    experiment_name=\"rauq_no_repair\",\n",
    "    trigger=\"rauq\",\n",
    "    repair=\"none\",\n",
    "    cot_policy=\"none\",\n",
    "    cot_length=0,\n",
    "    rollback_depth=2,\n",
    "    rollback_mode=\"fixed\",\n",
    ")\n",
    "no_cot_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3 \u2014 RAUQ controller with rerank repair\n",
    "rerank_summary = run_controller_experiment(\n",
    "    experiment_name=\"rauq_rerank\",\n",
    "    trigger=\"rauq\",\n",
    "    repair=\"rerank\",\n",
    "    cot_policy=\"rauq\",\n",
    "    rollback_depth=2,\n",
    "    rollback_mode=\"fixed\",\n",
    ")\n",
    "rerank_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 4 \u2014 RAUQ controller with anchor-style rollback\n",
    "anchor_summary = run_controller_experiment(\n",
    "    experiment_name=\"rauq_cot_anchor\",\n",
    "    trigger=\"rauq\",\n",
    "    repair=\"cot\",\n",
    "    cot_policy=\"rauq\",\n",
    "    rollback_depth=3,\n",
    "    rollback_mode=\"anchor\",\n",
    ")\n",
    "anchor_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 5 \u2014 RAUQ controller with fixed-length CoT (20 tokens)\n",
    "fixedcot_summary = run_controller_experiment(\n",
    "    experiment_name=\"rauq_cot_fixed20\",\n",
    "    trigger=\"rauq\",\n",
    "    repair=\"cot\",\n",
    "    cot_policy=\"max20\",\n",
    "    cot_length=20,\n",
    "    rollback_depth=2,\n",
    "    rollback_mode=\"fixed\",\n",
    ")\n",
    "fixedcot_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 6 \u2014 RAUQ controller with lighter attention weighting (alpha = 0.1)\n",
    "# Note: Reuses the baseline theta; rerun threshold calibration if you expect to productionize this variant.\n",
    "alpha_low_summary = run_controller_experiment(\n",
    "    experiment_name=\"rauq_cot_alpha0_1\",\n",
    "    trigger=\"rauq\",\n",
    "    repair=\"cot\",\n",
    "    cot_policy=\"rauq\",\n",
    "    rollback_depth=2,\n",
    "    rollback_mode=\"fixed\",\n",
    "    alpha=0.1,\n",
    ")\n",
    "alpha_low_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 7 \u2014 RAUQ controller with heavier attention weighting (alpha = 0.5)\n",
    "# Note: Reuses the baseline theta; calibrate a dedicated threshold for a fairer comparison if needed.\n",
    "alpha_high_summary = run_controller_experiment(\n",
    "    experiment_name=\"rauq_cot_alpha0_5\",\n",
    "    trigger=\"rauq\",\n",
    "    repair=\"cot\",\n",
    "    cot_policy=\"rauq\",\n",
    "    rollback_depth=2,\n",
    "    rollback_mode=\"fixed\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "alpha_high_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate experiment summaries for quick comparison\n",
    "experiment_summaries = {\n",
    "    \"baseline_greedy\": greedy_summary,\n",
    "    \"rauq_cot_baseline\": baseline_summary,\n",
    "    \"rauq_no_repair\": no_cot_summary,\n",
    "    \"rauq_rerank\": rerank_summary,\n",
    "    \"rauq_cot_anchor\": anchor_summary,\n",
    "    \"rauq_cot_fixed20\": fixedcot_summary,\n",
    "    \"rauq_cot_alpha0_1\": alpha_low_summary,\n",
    "    \"rauq_cot_alpha0_5\": alpha_high_summary,\n",
    "}\n",
    "\n",
    "print(\"name\".ljust(26), \"accuracy\", \"avg_triggers\", \"avg_tokens\", \"avg_latency\", sep=\" | \")\n",
    "for name, summary in experiment_summaries.items():\n",
    "    if summary is None:\n",
    "        continue\n",
    "    print(\n",
    "        name.ljust(26),\n",
    "        f\"{summary['accuracy']:.4f}\",\n",
    "        f\"{summary['avg_triggers']:.2f}\",\n",
    "        f\"{summary['avg_generated_tokens']:.1f}\",\n",
    "        f\"{summary['avg_latency_sec']:.2f}s\",\n",
    "        sep=\" | \",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}